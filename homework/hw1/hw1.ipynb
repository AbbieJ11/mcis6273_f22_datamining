{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["\\begin{center}\n", "\\begin{huge}\n", "MCIS6273 Data Mining (Prof. Maull) / Fall 2022 / HW1\n", "\\end{huge}\n", "\\end{center}\n", "\n", "| Points <br/>Possible | Due Date | Time Commitment <br/>(estimated) |\n", "|:---------------:|:--------:|:---------------:|\n", "| 20 | Monday, Sep 19 @ Midnight | _up to_ 24 hours |\n", "\n", "\n", "* **GRADING:** Grading will be aligned with the completeness of the objectives.\n", "\n", "* **INDEPENDENT WORK:** Copying, cheating, plagiarism  and academic dishonesty _are not tolerated_ by University or course policy.  Please see the syllabus for the full departmental and University statement on the academic code of honor.\n", "\n", "## OBJECTIVES\n", "* Learn more about data science tools in the wild for practitioners\n", "\n", "* Perform basic data engineering\n", "\n", "* Manipulate and analyze the data from NHTSA\n", "\n", "## WHAT TO TURN IN\n", "You are being encouraged to turn the assignment in using the provided\n", "Jupyter Notebook.  To do so, make a directory in your Lab environment called\n", "`homework/hw0`.   Put all of your files in that directory.  Then zip that directory,\n", "rename it with your name as the first part of the filename (e.g. `maull_hw0_files.zip`), then\n", "download it to your local machine, then upload the `.zip` to Blackboard.\n", "\n", "If you do not know how to do this, please ask, or visit one of the many tutorials out there\n", "on the basics of using zip in Linux.\n", "\n", "If you choose not to use the provided notebook, you will still need to turn in a\n", "`.ipynb` Jupyter Notebook and corresponding files according to the instructions in\n", "this homework.\n", "\n", "\n", "## ASSIGNMENT TASKS\n", "### (30%) Learn more about data science tools in the wild for practitioners \n", "\n", "Lot's of interesting things are going on in the data \n", "science landscape.  While we won't be talking about\n", "visualization in detail until later in the semester\n", "this podcast hits a number of issues on the head,\n", "not only with visualization, but with data science\n", "writ large and with the ability for large numbers \n", "of \"data geeks\" from all ranges of technical skill\n", "and ability can come together and work out data \n", "explorations together.\n", "\n", "You will listen to the podcast [FLOSS Weekly](https://twit.tv/shows/floss-weekly) **#677 April \n", "20, 2022**: _Open Source and Data Visualization_ featuring an interview with \n", "Melody Meckfessel from [Observable (https://observablehq.com)](https://observablehq.com) \n", "who goes into detail about the platform, the purpose and the \n", "community that has emerged there.\n", "\n", "You can listen to / watch the show from one of the links below (there are others (i.e. Apple Podcasts) if you search):\n", "\n", "* (main page) [TWiT: FLOSS WEEKLY 677 OPEN SOURCE AND DATA VISUALIZATION](https://twit.tv/shows/floss-weekly/episodes/677?autostart=false)\n", "* (mp3 direct) [TWiT: MP3 file direct download](https://twit.cachefly.net/audio/floss/floss0677/floss0677.mp3)\n", "* (youtube) [YouTube: FLOSS Weekly #677](https://www.youtube.com/watch?v=DFZx2i34vOU)\n", "\n", "**&#167; Task:**  Listen to the podcast / watch the video and write a 3-5 sentence \n", "_reaction_ to the podcast.  State in your own words what you learned,\n", "what expanding your knowledge of the topic and what you found _interesting_\n", "about the information you received.\n", "\n", "\n", "**&#167; Task:**  Go to [observablehq.com](https://observablehq.com) and find ONE project.\n", "With the one project please enumerate the following:\n", "\n", "(a) please give the title, URL and description of the project on Observable,\n", "(b) describe the datasets used in the project (you can just provide 1 sentence summary of the dataset),\n", "(c) provide a brief description of the visualizations used (1 sentence),\n", "(d) describe why you found this dataset / project interesting (no more than 2 sentences).\n", "\n", "\n", "\n", "### (40%) Perform basic data engineering \n", "\n", "As data scientists, especially in a small organization, you will\n", "be tasked with getting data prepared for analysis.  The scope of this\n", "task may be broad -- you might have to wait for the data, you may\n", "need to clean the data or build subsets of it for consumption\n", "by yourself (or others).\n", "\n", "It will, nonetheless, be a part of your life -- and it will consume\n", "time and energy.\n", "\n", "In this part of the assignment we will be working with a dataset from\n", "the [US National Highway Transportation Safety Administration (NHTSA)](https://nhtsa.gov/)\n", "which provides a number of safety and administrative functions\n", "to the US national roadways.  One of the important functions of \n", "this organizations is to collect data about a wide array of\n", "safety issues, one of which is data collection of accidents in \n", "all US states and territories.\n", "\n", "The data, at least in electronic form, goes back to the mid-1970s \n", "and as we will see, gets more detailed over the years.\n", "\n", "Our focus for this assignment will be on the [FARS (Fatal Accident Reporting System)](https://www.nhtsa.gov/research-data/fatality-analysis-reporting-system-fars)\n", "because we are keenly interested in some very specific trends\n", "in fatal accidents, which we will learn about shortly.\n", "\n", "All of the data in FARS system can be downloaded.  For example,\n", "you can go [here](https://www.nhtsa.gov/research-data/fatality-analysis-reporting-system-fars)\n", "and access any number of years of data, which is *exactly* what we will be doing in an automated way.\n", "\n", "**IMPORTANT RESOURCES**\n", "\n", "* you will most certainly need to refer to the [FARS Analytical User's Manual](https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/813254) \n", "to understand some of the codes being used within the data files.\n", "\n", "**&#167; Task:**  (_Data Extraction, Selection and Transformation_)\n", "\n", "You will be downloading a large amount of data from FARS \n", "in preparation for analysis.  For this part, you will\n", "create a Jupyter notebook, which will go to the\n", "FTP files of NHTSA. \n", "\n", "Your Python program / notebook must do the following:\n", "\n", "(1) fetch to your local file system every 5th year of data\n", "starting with 1975 to 2020.  That is you will **automate**\n", "downloading the `.zip` files at the static FTP site and\n", "store it locally for 1975, 1980, 1985, ... 2015 and 2020.\n", "\n", "(2)  Once you have fetched the `.zip` files locally, you will\n", "then unzip them automatically to a folder corresponding to their\n", "year.  Thus you will have 10 folders `\"./1975\"`,`\"./1980\"`, ... which\n", "will contain the contents of their correspoding `.zip` file\n", "from the NHTSA.  \n", "\n", "(3)  You will notice each of these folders contain many `.csv` files.  You will \n", "create a folder at the same level as your notebook called `accident_all_years`\n", "and you will copy (not move), each of the yearly files (i.e. `\"1975/accident.csv\"`) to \n", "the folder `\"accident_all_years/1975_accident.csv\"`.  The new folder will contain\n", "just the `accident.csv` files for all years downloaded.\n", "\n", "\n", "You will (minimally) need to study the following Python libraries to complete this task:\n", "\n", "* [requests](https://requests.readthedocs.io/)\n", "* [zipfile](https://docs.python.org/3/library/zipfile.html)\n", "* [os](https://docs.python.org/3/library/os.html)\n", "\n", "You do not want to overthink this **and use functions** to perform the mundane\n", "automation of this assignment.\n", "\n", "\n", "\n", "### (30%) Manipulate and analyze the data from NHTSA \n", "\n", "Now that we have data, let's analyze it.\n", "\n", "There are so many questions that can be asked of this data \n", "but something of extreme interest to a lot of people is \n", "the impact driving under the influence of \n", "alcohol has on road safety.  Driving under the influence has been illegal\n", "for many decades (100 years ago cars existed, but were \n", "not yet as common as today so the rules of drinking and driving had to \n", "be made up as the number of drivers increased), \n", "but the limits defining \"legally\" \n", "under the influence have changed dramatically since the \n", "1970s.  One might consider one simple fact: more people \n", "owned and drove cars post WWII and by the 1970s many \n", "US households began owning _two_ cars, and as suburban \n", "sprawl emerged, the distances people were driving \n", "also increased.  Also realize the legal drinking age \n", "was 18 in many US states, until it was federally changed \n", "to 21 in the 1970s.\n", "\n", "Blood Alcohol Concentration (BAC) has usually been the \n", "standard to measure the level of alcohol in the blood \n", "if one is stopped and asked to perform an alcohol \n", "test  (implied consent is nearly universal in the US). \n", "The _federal_ \n", "BAC limit has _dropped 50%_ since the 70s\n", "from .15 to .08 today, but one state (Utah) has a limit of .05!\n", "\n", "We're going to build an analysis to ascertain fatality relationships \n", "with driving under the influence\n", "and really explore the historical trends since 1975.\n", "\n", "You will need to study the data a bit and understand \n", "what you're looking at, as the first part of this \n", "assignment will have you explore it in depth.\n", "      \n", "<!--\n", "  * trend in deaths\n", "  * ratio of alchohol fatalities \n", "  * \n", "-->\n", "\n", "**&#167; Task:**  (_Descriptive Statistics / Exploratory Data Analysis_) \n", "\n", "One of the first questions we might like to ask of the data, \n", "is about the number of fatalities, and then \n", "understand how many of those fatalities involved a driver who was \n", "over the legal limit for alcohol consumption (keeping in mind that \n", "limit changes over time).  \n", "\n", "You will use your data from the first part to answer the following:\n", "\n", "(a) What is the overall number fatal accidents for the entire dataset \n", "period from 1975-2020?  (**Note:** the denomenator is the total number of accidents)\n", "\n", "(b) How many people died over that period?  How many _total_ people were involved (fatal and non-fatal)?\n", "\n", "(c) What proportion of accidents occurred between 9pm and 4am (overnight)?\n", "\n", "(d) What proportion of accidents occurred when the weather was snowy?\n", "\n", "(d) Build a line graph that shows the total fatalities by year (using the \n", "dataset with just every 5 years of data).\n", "\n", "(e) Make a general statement about what you observe in the line graph.\n", "\n", "To be successful you will need to study the following:\n", "\n", "* Pandas [Dataframe.groupby()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html), include [sum()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.sum.html) and [count()](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.count.html)\n", "* Pandas selection (include [Dataframe.loc[]](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html))\n", "* Pandas [Dataframe.plot()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html)\n", "\n", "\n", "**&#167; Task:**  (_Descriptive Statistics / Exploratory Data Analysis_) \n", "\n", "Now that we have a feel for the data, let's go ahead and dig a little deeper\n", "into analysis at the state level. \n", "\n", "On [page 45 of the FARS manual](https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/813254) you will see state codes.  These will be useful soon.\n", "\n", "Of course, we would hope that over the years states recognized\n", "that there were higher accidents and deaths due to drivers under\n", "the influence of alcohol, and thus tried to _do_ something\n", "about it.  Much of what has been done over the past 45 years \n", "has been through awareness, police force training, public messaging \n", "and stricter laws.\n", "\n", "You will need to cozy up with the data once more and answer the following \n", "questions.  All of your answers will need to be coded in the Jupyter notebook\n", "using Python and Pandas:\n", "\n", "(a) From 1975-2020, what was the _average_ (mean) rate of \n", "fatal accidents which involved an intoxicated driver?  This \n", "would be over _all_ states.\n", "\n", "(b) In 1975 which 5 states had the highest rate of fatal accidents \n", "involving an intoxicated driver?  Which 5 had the least?  Please \n", "list the states and the rates in a table in the notebook.\n", "\n", "(c) By 1990, how much had the top and bottom 5 changed (if at all)?\n", "\n", "(d) What was the average (mean) rate in 2020?\n", "\n", "(e) Plot a graph with the top and bottom five states, showing just the rate \n", "over time (from 1975-2020).  You can plot these in two graphs (top 5 and bottom 5 do \n", "not have to be in the same graph).\n", "\n", "(f)  What is your interpretation of the trend -- pretend you have no knowledge \n", "about the changes in law, changes in BAC thresholds or changes in \n", "public messaging about DUIs. \n", "\n", "To be successful you will need to study the following:\n", "\n", "* Pandas [Dataframe.groupby().agg()](https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.core.groupby.DataFrameGroupBy.agg.html)\n", "* Pandas [Dataframe.head()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html) and [Dataframe.tail()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.tail.html)\n", "\n", "\n", "**&#167; Task:**  (_Distance Metrics / Exploratory Data Analysis_) \n", "\n", "In lecture, we were introduced to a number of distance \n", "metrics, and also to normalization and data scaling.\n", "\n", "In this last and final part of the assignment, we want \n", "to begin to understand how these metrics form the basis \n", "of algorithms such as K-Means and others -- but we will \n", "do it from the ground up and get a feel for the \n", "intuition behind clustering, which will be in the next\n", "assignment.\n", "\n", "What we are interested in doing is creating a similarity \n", "matrix so that we might visually see which data are most\n", "similar to one another.  This makes it much easier to \n", "then perform grouping of data that are \"near\" one \n", "another, then by studying the characteristics\n", "that make up these groups, we can understand \n", "the underlying composition of the emergent clusters.\n", "\n", "In this part we will perform an ad hoc \n", "similarity analysis, then we will use \n", "PCA to determine which features might be the\n", "most useful and re-run the similarity analysis \n", "with those features.\n", "\n", "(a) _You will create a subset of the data over all \n", "years._  \n", "\n", "* include 5000 random rows of data \n", "* restrict data only to the following columns:\n", "\n", "```text\n", "    'STATE',\n", "    'MONTH',\n", "    'DAY',\n", "    'YEAR',\n", "    'HOUR',\n", "    'PERSONS',\n", "    'MAN_COLL',\n", "    'LGT_COND',\n", "    'WEATHER',\n", "    'SCH_BUS',\n", "    'FATALS',\n", "    'DAY_WEEK',\n", "    'DRUNK_DR',\n", "```\n", "* you may need to eliminate rows with `NaN` data to simplify the analysis\n", "\n", "(b) Scale the data from part (a) such that all values are between 0 and 1.\n", "(c) Compute the distance metric of all 5000 values using _Euclidean_ distance and build a distance table.\n", "(d) Pick 2 random rows from your original 5000 and find the 20 nearest neighbors  of each .  Use a \n", "single sentence to describe the two collections of 20.  You can pretend these each represent a \n", "cluster.  You must show full work in your notebook.  You may also want to optionally bring all \n", "the original columns back (i.e. they contain years, which might be helpful)\n", "\n", "\n", "To be successful you may need to study the following:\n", "\n", "* SciKitLearn [preprocessing.MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html?highlight=minmaxscaler#sklearn.preprocessing.MinMaxScaler)\n", "* SciKitLearn [DistanceMetric.get_metric('euclidean')](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DistanceMetric.html?highlight=get_metric#sklearn.metrics.DistanceMetric.get_metric)\n", "* SciKitLearn [DistanceMetric.pairwise()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DistanceMetric.html?highlight=get_metric#sklearn.metrics.DistanceMetric.pairwise)\n", "* *or* SciPy [scipy.spatial.distance.cdist](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html)\n", "* Pandas [Dataframe.apply()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html)\n", "\n", "\n", "**&#167; Task:**  (_PCA / Exploratory Data Analysis_) \n", "\n", "The final part of this assignment is to explore dimensionality\n", "reduction a bit.  We talked about principal components analysis (PCA)\n", "in the readings and in lectures, but now we're going to\n", "apply it to the dataset. \n", "\n", "Recall, that what we're after is a reduction in the feature space \n", "such that the relevant components are preserved and are capable \n", "of representing the majority of the data.  Doing so greatly \n", "reduces a number of computation concerns, especially when \n", "dimensions are large and in reality there may only be a few \n", "dominant features.\n", "\n", "We will only go into PCA a little so you can get a flavor for the \n", "technique, but the curious would be advised to dig deeper, as there \n", "are a number of interesting variations that may be useful to you  in \n", "the future.\n", "\n", "In this part, simply use PCA to discover _how many_ components \n", "are necessary to represent the data.  \n", "\n", "(a) How many components are necessary to capture 90% of the \n", "data in the datast from the prior section.   Show the\n", "plot of the cummulative variance using [PCA.explained_variance_ratio_](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)\n", "\n", "(b) Please list which feature dominates the first component.  To do this, you will need \n", "to get the feature with the largest value, which corresponds to the column of the \n", "feature you are looking for, requiring a little backtracking.\n", "\n", "\n", "You may study the following resources (and more):\n", "\n", "* Scikit-Learn [PCA documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)\n", "* Medium articles by Ruksan Pramoditha \n", "  * [Statistical and Mathematical Concepts behind PCA](https://medium.com/data-science-365/statistical-and-mathematical-concepts-behind-pca-a2cb25940cd4)\n", "  * [Principal Components Analysis (PCA) with Scikit-Learn](https://towardsdatascience.com/principal-component-analysis-pca-with-scikit-learn-1e84a0c731b0)\n", "\n", "\n", "\n"]}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python [default]", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.1"}, "toc": {"colors": {"hover_highlight": "#DAA520", "navigate_num": "#000000", "navigate_text": "#333333", "running_highlight": "#FF0000", "selected_highlight": "#FFD700", "sidebar_border": "#EEEEEE", "wrapper_background": "#FFFFFF"}, "moveMenuLeft": true, "nav_menu": {"height": "12px", "width": "252px"}, "navigate_menu": true, "number_sections": false, "sideBar": true, "threshold": "1", "toc_cell": false, "toc_section_display": "block", "toc_window_display": true, "widenNotebook": false}}, "nbformat": 4, "nbformat_minor": 0}